{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLSDXlKPnMHS"
      },
      "source": [
        "# Classification with Logistic regression and SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f8YI-o1nSOi"
      },
      "source": [
        "**What you will learn today**: You will learn how to implement a classifier. First we will analyze a toy example in which we will use a logistic classifier coded from scratch. Once we understood the logic behind the algorithm we will use the black box Sklearn one for recognizing hand-written digits. You will also implement SVM classifier on a different dataset to get familiar with this important family of algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qca66JIEnlKt"
      },
      "source": [
        "# 1) Logistic regression from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVCVcDcu0vx"
      },
      "source": [
        "Let us generate a synthetic dataset using a multivariate Gaussian distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCWmPmI8nMHU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(0)\n",
        "num_observations = 500\n",
        "\n",
        "x1 = np.random.multivariate_normal([0, 0], [[1, .5],[.5, 1]], num_observations)\n",
        "x2 = np.random.multivariate_normal([1, 4], [[1, .8],[.8, 1]], num_observations)\n",
        "\n",
        "dataset = np.vstack((x1, x2)).astype(np.float32)\n",
        "labels = np.hstack((np.zeros(num_observations),np.ones(num_observations)))\n",
        "\n",
        "dataset[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFRfvxvru0vx"
      },
      "source": [
        "Let's plot our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35zOhDW_u0vx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,8))\n",
        "plt.xlim(-3,4)\n",
        "plt.scatter(dataset[:, 0], dataset[:, 1],c = labels, alpha = .4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GOOJwxFnMHW"
      },
      "source": [
        "Our goal is to use a logistic function to fit our dataset. In this case:\n",
        "$$P_{\\rm model}(y_i|{\\vec w} \\cdot {\\vec x_i}) = \\frac {e^{({\\vec w} \\cdot {\\vec x_i})y_i}}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}$$\n",
        "such that\n",
        "$$P_{\\rm model}(y_i=1)  = \\frac {\\exp{({\\vec w} \\cdot {\\vec x_i})}}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}  = \\frac {1}{1+\\exp(-{\\vec w} \\cdot {\\vec x_i})}~~~ \\text{and}~~~ P_{\\rm model}(y_i=0)  = \\frac {1}{1+\\exp({\\vec w} \\cdot {\\vec x_i})}  = \\frac {\\exp{(-{\\vec w} \\cdot {\\vec x_i})}}{1+\\exp(-{\\vec w} \\cdot {\\vec x_i})}$$\n",
        "\n",
        "\n",
        "Now, we can either use the cross-entropy loss and write:\n",
        "$$\\mathcal{L}(\\vec{w}) = - \\sum_{i=1}^n \\sum_{y_i=0,1} y_i \\log P_{\\rm model}(y_i) +  (1-y_i) \\log (1 - P_{\\rm model}(y_i)) $$\n",
        "or equivalently write a maximum-likelihood problem with\n",
        "$$\n",
        "\\mathcal{L}(\\vec{w}|\\vec{\\bm{x}},\\bm{y}) \\propto \\sum_i \\log P_{\\rm model}(y_i|{\\vec w} \\cdot {\\vec x_i})\n",
        "$$\n",
        "In both cases, the problem boils down to minimizing the following loss:\n",
        "$$\\mathcal{L}(\\vec {w}) =  \\sum_{i=1}^n - y_i {\\vec {w}} \\cdot {\\vec x}_i  + \\log{(1+\\exp({\\vec w} \\cdot {\\vec x}_i ))} $$\n",
        "\n",
        "Let us implement these function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfEQgF6wnMHX"
      },
      "outputs": [],
      "source": [
        "def sigmoid(scores):\n",
        "    return 1 / (1 + np.exp(-scores))\n",
        "def log_loss(features, target, weights):\n",
        "    scores = np.dot(features, weights)\n",
        "    ll = np.sum( -target*scores + np.log(1 + np.exp(scores)) )\n",
        "    return ll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYi3OexonMHX"
      },
      "source": [
        "In order to perform optimization, we need to compute the gradient and perform gradient descent. Here we have:\n",
        "\n",
        "$$\\nabla_{\\vec w} \\mathcal{L}(\\vec w) =  \\sum_{i = 1}^n - y_i  {\\vec x}_i  + {\\vec x}_i  \\frac{\\exp({\\vec w} \\cdot {\\vec x}_i )}{(1+\\exp({\\vec w} \\cdot {\\vec x}_i ))} = - \\sum_{i = 1}^n {\\vec x}_i^T (y_i - P(y_i=1)) $$\n",
        "\n",
        "We can now write the  logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sX_8k3UbDhG"
      },
      "source": [
        "##### **Exercise 1**\n",
        "\n",
        "##### Try writing Gradient Descent using this Loss, as we did in the previous exercise session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDBFQxQhnMHX"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(features, target, num_steps, learning_rate):\n",
        "    weights = np.zeros(features.shape[1])\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        scores = np.dot(features, weights)\n",
        "        predicton = sigmoid(scores)\n",
        "        diff = target - predicton\n",
        "        gradient = -np.dot(features.T, diff)\n",
        "        weights = weights - learning_rate * gradient\n",
        "\n",
        "        if step % 10000 == 0:\n",
        "            print(step, log_loss(features, target, weights))\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABDhFkSnMHY"
      },
      "source": [
        "We use the usual trick of adding a \"1\" to the data so that we can actually linear fitting, and call the regression function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260ZSKYNu0vy"
      },
      "outputs": [],
      "source": [
        "intercept = np.ones((dataset.shape[0], 1))\n",
        "data_with_intercept = np.hstack((intercept, dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHnoEVSu0vy"
      },
      "source": [
        "We then run the actual GD algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-R1PqDjnMHY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "weights = logistic_regression(data_with_intercept, labels, num_steps = 300000, learning_rate = 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uklaAfifnMHZ"
      },
      "source": [
        "Now we can plot the prediction from our model, and check how good they are on the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_n7Dt78nMHZ"
      },
      "outputs": [],
      "source": [
        "def line(x,a,b,c):\n",
        "    return -x*b/c-a/c\n",
        "def myline(x):\n",
        "    a = weights[0]\n",
        "    b = weights[1]\n",
        "    c = weights[2]\n",
        "    return -x*b/c-a/c\n",
        "\n",
        "final_scores = np.dot(data_with_intercept, weights)\n",
        "preds = np.round(sigmoid(final_scores))\n",
        "\n",
        "print('Accuracy: {0}'.format((preds == labels).sum().astype(float) / len(preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmoqMdyMnMHZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (14, 8))\n",
        "plt.scatter(dataset[:, 0], dataset[:, 1],\n",
        "            c = (preds == labels) , alpha = .8, s = 50)\n",
        "plt.xlim([-3,4])\n",
        "plt.plot([-3,4],[myline(-3),myline(4)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHKgN38BnMHa"
      },
      "source": [
        "In high dimensional statistics problems, it is often the case that the number of points are of the same order as the dimension. Let us mimick this by using fewer points in dimension 2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3wcBp5wnMHa"
      },
      "outputs": [],
      "source": [
        "num_observations = 50\n",
        "\n",
        "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
        "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
        "\n",
        "dataset = np.vstack((x1, x2)).astype(np.float32)\n",
        "labels = np.hstack((np.zeros(num_observations),\n",
        "                              np.ones(num_observations)))\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.xlim([-3,4])\n",
        "plt.scatter(dataset[:, 0], dataset[:, 1], c = labels, alpha = .4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZNt5mLFnMHa"
      },
      "source": [
        "Of course, we could just redo our fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsfYxv9KnMHb"
      },
      "outputs": [],
      "source": [
        "intercept = np.ones((dataset.shape[0], 1))\n",
        "data_with_intercept = np.hstack((intercept, dataset))\n",
        "\n",
        "weights = logistic_regression(data_with_intercept, labels,num_steps = 100000, learning_rate = 1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyLhOxSvnMHb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def myline(x):\n",
        "    return line(x,weights[0],weights[1],weights[2])\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.xlim([-3,4])\n",
        "plt.scatter(dataset[:, 0], dataset[:, 1], c = labels, alpha = .4)\n",
        "plt.plot([-3,4],[myline(-3),myline(4)])\n",
        "\n",
        "final_scores = np.dot(data_with_intercept, weights)\n",
        "preds = np.round(sigmoid(final_scores))\n",
        "print('Accuracy: {0}'.format((preds == labels).sum().astype(float) / len(preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nid2dpiqtnm"
      },
      "source": [
        "# 2) Logistic regression on real dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsF6A63PrKJV"
      },
      "source": [
        "Let us use the fantastic notebooks from https://physics.bu.edu/~pankajm/MLnotebooks.html, a terrific book for ML newcomers especially if they come from physics. We will consider again our friend MNIST, remember the lecture on KNN? First a bit of history on this famous dataset:\n",
        "\n",
        "\n",
        "The MNIST classification problem is one of the classical ML problems for learning classification on high-dimensional data with a fairly sizable number of examples (60000). Yann LeCun and collaborators collected and processed $70000$ handwritten digits (60000 are used for training and 10000 for testing) to produce what became known as one of the most widely used datasets in ML: the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. Each handwritten digit comes in a grayscale square image in the shape of a $28\\times 28$ pixel grid. Every pixel takes a value in the range $[0,255]$, representing $256$ nuances of the gray color. The problem of image classification finds applications in a wide range of fields and is important for numerous industry applications of Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcGgKgZrfuD"
      },
      "source": [
        " ### SoftMax regression:\n",
        " Before we consider two possible labels for our data, you can easily guess that it is amenable to have more than one in this case.\n",
        "\n",
        " Do not worry! We can easily generalize what we said before.\n",
        "\n",
        "\n",
        "We will use SoftMax regression, which can be thought of as a statistical model which assigns a probability that a given input image corresponds to any of the 10 handwritten digits. The model is a generalization of the logistic regression and reads:\n",
        "$$\n",
        "p(y_i=j|\\vec{x}_i;\\vec{w}) = \\frac{e^{-\\vec{w}_j^T \\vec{x}}}{\\sum_{k=0}^9 e^{-\\vec{w}_k^T\\vec{x} }},\n",
        "$$\n",
        "Where $p(y_i=j|\\vec{x}_i;\\vec{w})$ is the probability that input $\\vec{x}_i$ is the $j$-th digit, $j\\in[0,9]$.\n",
        "The model also has 10 weight vectors $\\vec{w}_j$ which we will train below. Finally, one can use this information for prediction by taking the value of $y_i$ for which this probability is maximized:\n",
        "\\begin{align}\n",
        "y_{pred}=\\arg\\max_i p(y=i|\\vec{x})\n",
        "\\end{align}\n",
        "\n",
        "First thing to do is to import the dataset and preprocess the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOdvFJG4tu27"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_openml # MNIST data\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# Turn down for faster convergence\n",
        "train_size = 60000\n",
        "test_size = 10000\n",
        "\n",
        "### load MNIST data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGyXynRLu0vz"
      },
      "outputs": [],
      "source": [
        "X = np.asarray(X)\n",
        "y = np.asarray(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKDGn265Uf_4"
      },
      "source": [
        "Let's plot an image to see how it looks like with plt.imshow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP8PLOdX4tff"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[45,:].reshape(28,28))\n",
        "y[45]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkGXOWzXdTN"
      },
      "source": [
        "We shuffle the data and we do the test-train splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClqVsH714o5W"
      },
      "outputs": [],
      "source": [
        "# shuffle data\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "# pick training and test data sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=train_size,test_size=test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX5Q1D3SXtIP"
      },
      "source": [
        "We preproccess the data and use StandardScaler to have zero mean and unit variance. Pay attention that we do that only on the training set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXbbimCzXtUe"
      },
      "outputs": [],
      "source": [
        "# scale data to have zero mean and unit variance [required by regressor]\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "#X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36PDxvTtt4oI"
      },
      "source": [
        "**Exercise**\n",
        " 1. You need to learn how to read a Python documentation. Search on the internet how to implement softmax regression.\n",
        " 2. Fix the value of the regularization to be $10^{-5}$. Fit the data and compute two quantities:\n",
        "   - Sparsity of the weights (percentage of non-zero weights)\n",
        "   - Score (i.e. accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwly0qE-rfuF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "coef = np.zeros((10, X_train.shape[1])) #has to be changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcpGP1dpus3o"
      },
      "source": [
        "##### Let us see how we can actually plot the weights against the pixels!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Z5Fy7srfuH"
      },
      "outputs": [],
      "source": [
        "# plot weights vs the pixel position\n",
        "plt.figure(figsize=(16, 8))\n",
        "scale = np.abs(coef).max()\n",
        "for i in range(10):\n",
        "    l2_plot = plt.subplot(2, 5, i + 1)\n",
        "    l2_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
        "                   cmap=plt.cm.Greys, vmin=-scale, vmax=scale)\n",
        "    l2_plot.set_xticks(())\n",
        "    l2_plot.set_yticks(())\n",
        "    l2_plot.set_xlabel('Class %i' % i)\n",
        "plt.suptitle('classification weights vector $w_j$ for digit class $j$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDPu7XiMxfrs"
      },
      "source": [
        "# 3) SVM Classification on real dataset\n",
        "\n",
        "## Evaluated Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZZr4RxTvbHW"
      },
      "source": [
        "**Exercise**\n",
        " 1. Repeat what you did for softmax regression for an Support Vector Machine (SVM) linear classifier.\n",
        " 2. Go and search the Python doc for SVM classifier. Choose the linear one.\n",
        " 3. Fit the data using the default value of regularization and compare with an optimized value using CV."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "FoIL_TP6_Private.ipynb",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}